{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, InputLayer, LeakyReLU, Reshape, Flatten\n",
    "from keras.layers.pooling import MaxPooling1D, MaxPooling2D, AveragePooling1D, AveragePooling2D\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_index(fold_id, key='train', n=None):\n",
    "    \n",
    "    with open(os.path.join(DESTDIR, 'experiment_{:02d}.json'.format(fold_id)), 'r') as fdesc:\n",
    "        index = json.load(fdesc)[key]\n",
    "    \n",
    "    perm = np.random.permutation(len(index))\n",
    "    if n is not None:\n",
    "        perm = perm[:n]\n",
    "        \n",
    "    index = [index[_] for _ in perm]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    return librosa.logamplitude(X**2, ref_power=np.max, top_db=80)\n",
    "\n",
    "\n",
    "def load_data(index, n_harm=1):\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for item in index:\n",
    "        with np.load(os.path.join(DESTDIR, 'features', '{}.npz'.format(item['filename']))) as data:\n",
    "            if n_harm is None:\n",
    "                n_harm = data['C'].shape[0]\n",
    "            X.append(preprocess(data['C'][:n_harm]))\n",
    "            Y.append(item['classID'])\n",
    "    \n",
    "    X = np.asarray(X).swapaxes(3, 1)\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DESTDIR = '/home/bmcfee/working/minst/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FOLD_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = load_index(FOLD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_index = load_index(FOLD_ID, key='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_HARM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = load_data(index, n_harm=N_HARM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xt, Yt = load_data(test_index, n_harm=N_HARM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer(input_shape=X.shape[1:], name='input'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(16, (4 - X.shape[3] + 1) * 5, 5,  bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "\n",
    "model.add(Conv2D(32, 3, 3,  bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Conv2D(64, 1, model.output_shape[2], bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "model.add(AveragePooling2D(pool_size=(model.output_shape[1], 1)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_fn = '/home/bmcfee/working/minst/models_{}.hdf5'.format(N_HARM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26610 samples, validate on 8870 samples\n",
      "Epoch 1/30\n",
      "26610/26610 [==============================] - 34s - loss: 0.3594 - acc: 0.9009 - val_loss: 0.2304 - val_acc: 0.9258\n",
      "Epoch 2/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.1002 - acc: 0.9722 - val_loss: 0.0710 - val_acc: 0.9789\n",
      "Epoch 3/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0652 - acc: 0.9815 - val_loss: 0.1097 - val_acc: 0.9631\n",
      "Epoch 4/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0484 - acc: 0.9859 - val_loss: 0.0484 - val_acc: 0.9859\n",
      "Epoch 5/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0410 - acc: 0.9881 - val_loss: 0.0369 - val_acc: 0.9869\n",
      "Epoch 6/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0308 - acc: 0.9909 - val_loss: 0.0686 - val_acc: 0.9791\n",
      "Epoch 7/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0298 - acc: 0.9911 - val_loss: 0.0712 - val_acc: 0.9751\n",
      "Epoch 8/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0266 - acc: 0.9918 - val_loss: 0.0386 - val_acc: 0.9885\n",
      "Epoch 9/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0233 - acc: 0.9933 - val_loss: 0.1059 - val_acc: 0.9657\n",
      "Epoch 10/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0194 - acc: 0.9948 - val_loss: 0.0195 - val_acc: 0.9936\n",
      "Epoch 11/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0185 - acc: 0.9942 - val_loss: 0.0479 - val_acc: 0.9838\n",
      "Epoch 12/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0173 - acc: 0.9947 - val_loss: 0.0235 - val_acc: 0.9915\n",
      "Epoch 13/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0140 - acc: 0.9961 - val_loss: 0.0271 - val_acc: 0.9913\n",
      "Epoch 14/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0172 - acc: 0.9948 - val_loss: 0.0312 - val_acc: 0.9887\n",
      "Epoch 15/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0108 - acc: 0.9972 - val_loss: 0.0286 - val_acc: 0.9906\n",
      "Epoch 16/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0128 - acc: 0.9961 - val_loss: 0.0464 - val_acc: 0.9864\n",
      "Epoch 17/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0121 - acc: 0.9960 - val_loss: 0.0287 - val_acc: 0.9903\n",
      "Epoch 18/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0215 - val_acc: 0.9936\n",
      "Epoch 19/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0100 - acc: 0.9971 - val_loss: 0.0257 - val_acc: 0.9910\n",
      "Epoch 20/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0104 - acc: 0.9961 - val_loss: 0.0781 - val_acc: 0.9771\n",
      "Epoch 21/30\n",
      "26610/26610 [==============================] - 33s - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0473 - val_acc: 0.9846\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, to_categorical(Y, nb_classes=12),\n",
    "                    callbacks=[EarlyStopping('val_loss', patience=10),\n",
    "                               ModelCheckpoint(weight_fn, save_best_only=True)],\n",
    "                    validation_split=0.25, batch_size=32, shuffle=True,\n",
    "                    nb_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(weight_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3417/3417 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1673523644309731, 0.60199004948959034]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(N_HARM)\n",
    "model.evaluate(Xt, to_categorical(Yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3417/3417 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.0875094977951831, 0.5405326308232844]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(N_HARM)\n",
    "model.evaluate(Xt, to_categorical(Yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3417/3417 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1312941890869497, 0.561018437208193]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(N_HARM)\n",
    "model.evaluate(Xt, to_categorical(Yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3417/3417 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4793655667208705, 0.67017851906420023]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(N_HARM)\n",
    "model.evaluate(Xt, to_categorical(Yt))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
